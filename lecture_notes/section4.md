# 4. Языковые модели и генерация текста
## 4.1. RNNs (Recurrent Neural Networks), рекуррентные нейросети

Рабочая лошадка про обработке последовательностей данных — рекуррентная нейросеть.

Основная идея — есть некоторое внутреннее состояние, которое изменяется с каждым приходом порции входных данных.

* Считываем последовательность символ за символом или слово за словом и применяем к элементу, например, линейное преобразование.
* Вычисляем новое состояние ячейки памяти исходя из предыдущего состояния (того, что раньше было в памяти) и нового входного элемента.
* Вычисляем предсказание (выход) в текущем состоянии.

Потенциально мощнее Свёрточных нейросетей, однако завязаны на последовательном выполнении итераций (чтобы узнать текущее состояние, надо знать все предыдущие). Поэтому возможности для распаралеливания, а значит и скорость обработки на графических ускорителях куда меньше.

Не так хорошо обучаются, наблюдаются проблемы затухания и взрыва градиента (vanishing & exploding gradient).

Рассмотрим простой пример одномерной нейросети, состоящей из небольшого числа слоёв и будем решать задачу классификации для текстов.

![1D-RNN](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/simple_rnn.PNG "Example RNN architecture")

Итак, на первом шаге `h0` мы имеем некоторый скаляр (рассматриваем одномерную сеть), это просто вход нейросети. На втором шаге имеем уже функцию от выхода на предыдущем шаге с некоторым весом `w` и входа на текущем шаге. `h_1=f(wh_0+z_1)`. На втором шаге уже имеем функцию от выхода предыдущего шага и текущего входа, то есть `h_2=f(wh_1+z_2)`. Это выражение можно раскрыть используя явное полное представление для состояния на шаге `h_1`. И так далее. В итоге мы получим довольно громоздкую композицию функций `h_i`.

Если теперь мы попытаемся пойти в обратную сторону (backprop), то по пути нам придётся взять производную от всех функций скрытых состояний `h_i` и веса будут возводиться в высокие степени. Таким образом приходим к двум проблемам -- взрывающийся и исчезающий (затухающий) градиент (exploding and vanishing gradient), поскольку производная от гиперболического тангенса, например, в промежутке от 0 до 1. Если перемножить между собой много таких штук, то получится очень маленькое число и значит до нашего некоторого слоя просто не дойдёт информация о ближних слоях. Градиент затухнет.

Проблема взрывающегося градиента проявляется, когда веса велики и набегают огромные значения, потому что вес возводится в высокую степень. Решается методом отсечения по пороговому значению и просто заменой на порог с соответствующим знаком. Проблема исчезающего градиента решается при помощи применения особых архитектур нейросетей -- блоков с т.н. long short-term memory (LSTM).

Основные идеи:

* сохранение постоянного объёма потока ошибки (сечения трубы)
* гейтинг (входной гейт, выходной гейт, направление изменения состояния, гейт памяти, гейт забывания, чувствительность).

Число параметров велико `8d^2+4d`, где d -- размерность скрытого состояния `h`. 

Упрощённая реализация идеи LSTM -- GRU (gated recurrent units). Число параметров `6d^2+6d`.

Ещё более простая модель -- Simple Recurrent Unit (SRU). Теперь каждая компонента вектора состояния на текущем шаге зависит только от одноименной с ней компонентой вектора предыдущего состояния (и не зависит от других). Примерно в 5-9 раз быстрее LSTM-ов при схожем качестве. Число параметров `3d^2+4d`.

## 4.2. Моделирование языка.

На входе -- токенизированный текст. Сэмпл из какого-то распределения, реализацию случайной величины. Оцениваем плотность вероятности встретить такую последовательность токенов в реальном тексте. Насколько фрагмент текста поход на настоящий текст на ЕЯ?

Оцениваем правдоподобность пользовательского текста (правильность употребления), TTS, машинный перевод. То есть типичный сценарий, когда применяется такой подход: мы переходим между доменами, переводим аудио в текст, а далее работаем внутри второго домена, оценивая, насколько "услышанный" нейросетью фрагмент текста похож на фрагмент текста на естественном языке, то есть, грубо говоря, оценивает, насколько полученный перевод человечен.

Например, языковая модель выбирает из двух услышанных словосочетаний "Let it be" & "Let eat bee" наиболее вероятное.

Важно при решении этой задачи научиться учиться без учителя. Активное направление -- перенос знаний, навыков (transfer learning). Основной смысл -- делаем общую модель, обучаем без учителя и затем подгоняем под конкретную задачу (fine-tuning).

Нам на вход даются тексты и мы их считаем реализациями случайной величины некоторого многомерного распределения. Итак, есть тексты `[t_1,t_2,...,t_k] ~ P(t_1,t_2,...,t_k)`. Применяем правило цепочки и разворачиваем вероятность: P(a,b,c)=P(a|b,c)P(b|c)P(c).

* Авторегрессионные модели -- один из подходов. Предсказываем вероятность следующего слова при наблюдении некоторого количества предыдущих слов. (autoregressive LM, ELMo, OpenAI GPT, XLNet).
* Предсказание слова в середине по остальным словам в тексте вокруг него. (Word2Vec CBOW, BERT, XLNet).

Рассмотрим простенькую авторегрессионную модель. Пусть `P(t_1,t_2,...,t_k)=П(P(t_i|t_{1:i-1}))`. При этом ограничим длину истории некоторым числом n сверху. А именно, скажем, что слова, которые встречались более n слов назад уже не влияют на вероятность слова на текущей позиции. Такие подходы и свойства называются Марковскими. Обучение заключается в подсчёте условных вероятностей.

Плюсы: простая реализация, быстрота.
Проблемы: 
* при увеличении n число N-грам растёт очень быстро, сложно обобщается и сжимается
* не можем работать с неизвестными словами (но если перейти к работе посимвольно, то отчасти ок, но вычислительная сложность растёт)
* не может учитывать сложного контекста

Прорывы:
* Skip-Gram -- предсказываем контекст по центральному слову (слева, справа)
* CBOW (continious bag of words) -- предсказываем центральное слово по контексту

Для каждого слова два вектора -- центральный и контекстный. Считаем по софтмаксу, применяем negative sampling, чтобы убыстрить. Функцией потерь выступает кросс-энтропия, число меток равно размеру словаря.

На обученной модели можно предсказывать правдоподобие текста по Skip-Gram.

Преимущества: 
* Простота
* Сжатое представление, можно использовать как признаки
* Возможность работы с большими словарями

Недостатки: 
* Out of vocabulary problem
* Один вектор для слова -- плохо, представление слов статично и не зависит от контекста

На помощь приходят авторегрессионные модели. RNNs, CNNs, LSTMs, Self-attention blocks.

Здесь обычно используем честный софтмакс, без негативного сэмплирования. Проблема OOV в этом случае решается взятием длины токена равного единице (то есть работаем с текстами на уровне одиночных символов). Но на практике такие модели (символьные) работают хуже, чем с токенами больших длин.

Один из подходов -- работать с представлениями на уровне целых слов, а вектора для неизвестных слов набирать из представлений N-грам. (FastText-embeddings).

Ещё применяют BPE (byte-pair encoding) -- типа сжатия без потерь для текстов, заменяя наиболее частые длинные участки новым символом. `aabc aabaa -> Xbc XbX -> Yc YX`. Сжали без потерь в более чем два раза.

Нейросетевые модели позволяют получать сжатые представления слов с учётом контекста. Оказывается, первые слои выделяют морфологию, следующие -- поверхностные, а последние слои -- семантические. Абстракция и паттернинг. Учитывают сложные контекст и легко работают с OOV-words.

В качестве минусов: дорого и сложно обучать, сложно сходятся (RNNs), длина учитываемого контекста слева ограничена.

Чтобы учитывать контекст слева и справа от слова, применяют двунаправленные модели типа ELMa. Можно пойти дальше и юзать BERT -- тогда учитываться будет весь текст, выбираться некоторая маска входных слов, они будут заменены на фиктивные токены, предполагается их независимость (что не вполне корректно) и делается предсказание. В целом работает ОК.

Лучше, чем BERT работает XL Net. Итак:

* ELMo, OpenAI GPT раскручивают цепочку только слева-направо.
* раскручивает в обоих направлениях, но плохо, что есть предположение условной независимости этих выдернутых слов (то есть сами по себе ОНИ появляются независимо друг от друга, что, конечно, не так).
* XLNet - двунаправленный порядок, но порядок факторизации меняется.

## 4.3. Семинар "генерация русских фамилий с помощью RNN"
Файлы семинара можно найти в github-репозитории: [вот здесь](https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task4_RNN_name_generator.ipynb).

## 4.4. Агрегация (пулинг), механизм внимания.
Как получить вектор, представляющий целое предложение или даже целый текст? Иными словами -- как агрегировать глобальный контекст?

На входе у нас есть матрица, предсталяющая текст. Число её строк равно числу слов в тексте, например. Число столбцов равно размеру эмбеддинга.

На выходе блоков агрегации получаем сжатую матрицу -- по длине или вообще один вектор. Агрегация применяется, когда нужно получить представление независимо от места встречаемости информации. Мы получаем важную семантическаую информацию (о чём идёт речь), но теряем информацию о расположении этой важной части в тексте (пространственную инфу).

Модули агрегации работают аналогично свёрткам -- скользящее окно. Виды пулинга -- усреднение и взятие максимума. Ширина учитываемого контекста при пулинге увеличивается, а число параметров нейросети (обучаемых параметров) не увеличивается, ведь в слоях агрегации нет обучаемых параметров.

Глобальный пулинг -- размер окна = длине последовательности, не последовательно уменьшается размерность, а сразу, одним шагом. Таким образом получаем векторное представление для всего текста.

Как правило глобальный пулинг используют ближе к последним слоям. Нет проблем с затуханием и взровом градиентов (потому что просто они не юзаются).

Проблемы: каждый канал агрегируется независимо, грубая и слабая операция преобразования.

__Механизм внимания__

Это идея умного пулинга. Более эффективно разделить операции оценки значимости (внимания) и передачи дальше.

![att-mech](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/attention_mechanism.PNG "Attention Mechanism")

На вход механизма внимания поступает матрица (NumOfWords x EmbSize). Сначала к вектору каждого слова применяем некоторую нейросеть с одним выходом -- оценкой значимости. Релевантность рассчитывается при помощи одной и той же нейросети, веса которой не зависят от каждого из входных слов. Далее нормируем на единицу софтмаксом и получаем вектор значимости. После этого мы перемножаем входную матрицу признаков и выход софтмакса -- вектор значимости. Таким образом получаем агрегированные в один вектор признаки входных слов в соответствии с их значимостью.

Помимо входной матрицы слов на вход механизма внимания опционально может подаваться вектор-запрос, опорный вектор для рассчёта релевантности слов.

Помимо входных данных компонентами являются: механизм расчёта релевантности (нейросеть, скалярное произведение) и механизм вычисления значений (те векторы, которые будут складываться с весами). И наконец механизм агрегации (softmax + sum).

Плюсы: 
* гибкость по сравнению с обычным avg/max pooling;
* результат вычисляется с учётом влияния векторов на друг друга (какие-то элементы векторов влияют на привлечение внимания, другие на передачу важной инфы);
* не затухает (а значит лучше для длинных последовательностей по сравнению с пулингом и рекуррентками);
* механизм внимания -- универсальная операция и на ней одной можно построить всю нейронку;
* можно частично интерпретировать решения, принимаемые нейросетью (то, на какие участки она обратила больше __внимания__, а на какие меньше).

![att-mech-preconv-arch](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/att_mech_preconv.PNG "Attention Mechanism Preconvolutional Architecture")

Часто для повышения гибкости системы можно применять предобработку входной матрицы, используя другую нейросеть. Причём веса нейросети для преобработки и веса сети для механизма внимания отличаются.

Зачастую бывает так, что релевантность слов оценивается относительно пользовательского запроса для конкретной потребности. Релевантность часто оценивается как произведение веткора-запроса на входную матрицу.

![att-mech-query-arch](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/att_mech_query.PNG "Attention Mechanism Query Architecture")

Иначе говоря, чем ближе вектора к вектору-запросу, тем более они значимы.

Можно скомбинировать все вышеперечисленные варианты и сделать прокаченную версию механизма внимания, а именно, задействовать в нём: 

* значения и ключи рассчитываются отдельно
* релевантность рассчитывается с учётом вектора-запроса

![att-mech-comb-arch](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/att_mech_comb.PNG "Attention Mechanism Combined Architecture")

Механизм вычисления релевантности может быть нелинейным. Ещё можно не брать вектор-запрос извне, а вычислить его самостоятельно по какому-нибудь правилу (например использовать пулинг). Тогда оценка значимости слова будет обусловлена на весь текст. 

Механизмы, которые используют в качестве ключей и значений слова из одного и того же текста и даже вектор-запрос генерируется из самого текста, носят навзание механизмов самовнимания (self-attention mechanism).

Используя механизм внимания можно сравнивать тексты. Пусть нам даны две матрицы с равными размерами представлений, но разной длиной.

![att-mech-sent-comp](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/att_mech_sent_comp.PNG "Attention Mechanism Combined Architecture")

Информация в такой матрице -- насколько `i` слово из первого текста похоже на `j` слово из второго. Можно такую матрицу нормализовать по двум измерениям. Если мы номрализуем по столбцам, то смысл будет таким: `i` столбец будет хранить представления __всего первого текста__ относительно конкретного __`i` слова второго текста__.

Такие представления крайне полезны в вопросно-ответных системах, когда первое предложение -- вопрос, а второе -- текст, в котором ищем ответ. И можем предсказать, содержится ли ответ на вопрос в тексте и где его искать (пространственно). Такая архитектура показана на рисунке выше.

Или же мы можем нормализовать матрицу по строкам. Тогда физический смысл будет зеркальным тому, что мы рассмотрели выше.

__SUMMARY:__ различные варианты пулинга, механизм внимания, варианты, применимость.

При этом важно отметить, что механизм внимания менее эффективен в вычислительном смысле (требуетя большее число операций).

## 4.5. Трансформер и self-attention

Перед тем как приступить к изучению недавно предложенного подхода к решению задач NLP, вспомним основные подходы и взглянем на них по-новому:

* RNNs -- обработка одного токена за шаг, обновление скрытого состояния. Посдедовательная работа (LSTM, GRU), с улучшенным параллелизмом (SimpleRU). Дорогой учёт контекста (O(n))
* CNNs -- параллельная обработка. Чтобы учесть контекст, нужно увеличивать число слоёв или размер ядра. Непрерывные свёртки дают сложность ~O(n/k). Прореженные (dilated) -- O(log_k(n)).

Проблемы:
* Чем больше шагов, тем выше вероятность потерять информацию по пути, а значит хуже учитывать далёкие зависимости
* RNN медленно учатся, но компактнее
* CNN норм учаться, но весомее по вычислениям

2017 год. Ребята из гугла -- механизм внимания, в целом как CNN, только быстрее и позволяет учитывать зависимости любой длины.

Простейшая схема работы механизма внимания:

![selfatt-algo](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/selfatt_algo.PNG "Self-attention algorythm simple scheme")

Реальная схема работы МВ:

![selfatt-algo-pro](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/selfatt_algo_pro.PNG "Self-attention algorythm real scheme")

Отличия заключаются в том, что используются различные проекции (осуществляется предобработка с применением различных нейросетей) для запросов, ключей и значений (queries, keys, values). Часто предобработка осуществляется при помощи домножения слева на матрицу (линейное преобразование).

Основные моменты:

![selfatt-algo-pro-tips](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/selfatt_algo_pro.PNG "Self-attention algorythm tips")

Недостатки механизма внимания:

* Потеря информации
* Только один аспект (если один токен получил большой вес, то все остальные -- намного меньший)

Авторы этого алгоритма заметили эти недостатки и предложили Multi-Head Self-Attention, где каждая голова -- это механизм внимания со своими весами. Это позволяет учесть множество аспектов, а не один, как в случае с одной головой. Такая реализация МВ используется в Transformer.

Чтобы адаптировать трансформер под определённые задачи (например, генерации текста, где необходимо учитывать только контекст слева), применяется маскирование -- квадратные матрицы, где элемент `(i,j)` равен единице, если влияние на `j` токен `i` токена необходимо учесть и 0 в противном случае. Мы затираем значения матрицы сходства (Logits), ставя в соответствующие элементы `-inf`, тем самым после нормализации софтмаксом они вырождаются в нули.

![mh-selfatt](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/mh_selfatt.PNG "Self-attention algorythm tips")

__Механизм внимания ничего не знает про порядок слов!__

Но нам, конечно, это важно! Применяем позиционное кодирование. Это вектор размерности эмбеддинга. Складываем эмбеддинг токена и эмбеддинг позиции. Чтобы не было сложностей с учётом более длинных текстов, чем есть в таблице эмбеддингов длин, будем использовать периодический сигнал -- синус или косинус (базис Фурье).

В оригинальной статье используются две похожих нейросети, которые, работая вместе, решают задачу машинного перевода -- энкодер и декодер, причём в последнем применяется маскирование для механизма внимания.