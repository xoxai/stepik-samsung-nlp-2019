# 4. Языковые модели и генерация текста
## 4.1. RNNs (Recurrent Neural Networks), рекуррентные нейросети

Рабочая лошадка про обработке последовательностей данных — рекуррентная нейросеть.

Основная идея — есть некоторое внутреннее состояние, которое изменяется с каждым приходом порции входных данных.

* Считываем последовательность символ за символом или слово за словом и применяем к элементу, например, линейное преобразование.
* Вычисляем новое состояние ячейки памяти исходя из предыдущего состояния (того, что раньше было в памяти) и нового входного элемента.
* Вычисляем предсказание (выход) в текущем состоянии.

Потенциально мощнее Свёрточных нейросетей, однако завязаны на последовательном выполнении итераций (чтобы узнать текущее состояние, надо знать все предыдущие). Поэтому возможности для распаралеливания, а значит и скорость обработки на графических ускорителях куда меньше.

Не так хорошо обучаются, наблюдаются проблемы затухания и взрыва градиента (vanishing & exploding gradient).

Рассмотрим простой пример одномерной нейросети, состоящей из небольшого числа слоёв и будем решать задачу классификации для текстов.

![1D-RNN](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/simple_rnn.PNG "Example RNN architecture")

Итак, на первом шаге `h0` мы имеем некоторый скаляр (рассматриваем одномерную сеть), это просто вход нейросети. На втором шаге имеем уже функцию от выхода на предыдущем шаге с некоторым весом `w` и входа на текущем шаге. `h_1=f(wh_0+z_1)`. На втором шаге уже имеем функцию от выхода предыдущего шага и текущего входа, то есть `h_2=f(wh_1+z_2)`. Это выражение можно раскрыть используя явное полное представление для состояния на шаге `h_1`. И так далее. В итоге мы получим довольно громоздкую композицию функций `h_i`.

Если теперь мы попытаемся пойти в обратную сторону (backprop), то по пути нам придётся взять производную от всех функций скрытых состояний `h_i` и веса будут возводиться в высокие степени. Таким образом приходим к двум проблемам -- взрывающийся и исчезающий (затухающий) градиент (exploding and vanishing gradient), поскольку производная от гиперболического тангенса, например, в промежутке от 0 до 1. Если перемножить между собой много таких штук, то получится очень маленькое число и значит до нашего некоторого слоя просто не дойдёт информация о ближних слоях. Градиент затухнет.

Проблема взрывающегося градиента проявляется, когда веса велики и набегают огромные значения, потому что вес возводится в высокую степень. Решается методом отсечения по пороговому значению и просто заменой на порог с соответствующим знаком. Проблема исчезающего градиента решается при помощи применения особых архитектур нейросетей -- блоков с т.н. long short-term memory (LSTM).

Основные идеи:

* сохранение постоянного объёма потока ошибки (сечения трубы)
* гейтинг (входной гейт, выходной гейт, направление изменения состояния, гейт памяти, гейт забывания, чувствительность).

Число параметров велико `8d^2+4d`, где d -- размерность скрытого состояния `h`. 

Упрощённая реализация идеи LSTM -- GRU (gated recurrent units). Число параметров `6d^2+6d`.

Ещё более простая модель -- Simple Recurrent Unit (SRU). Теперь каждая компонента вектора состояния на текущем шаге зависит только от одноименной с ней компонентой вектора предыдущего состояния (и не зависит от других). Примерно в 5-9 раз быстрее LSTM-ов при схожем качестве. Число параметров `3d^2+4d`.

## 4.2. Моделирование языка.

На входе -- токенизированный текст. Сэмпл из какого-то распределения, реализацию случайной величины. Оцениваем плотность вероятности встретить такую последовательность токенов в реальном тексте. Насколько фрагмент текста поход на настоящий текст на ЕЯ?

Оцениваем правдоподобность пользовательского текста (правильность употребления), TTS, машинный перевод. То есть типичный сценарий, когда применяется такой подход: мы переходим между доменами, переводим аудио в текст, а далее работаем внутри второго домена, оценивая, насколько "услышанный" нейросетью фрагмент текста похож на фрагмент текста на естественном языке, то есть, грубо говоря, оценивает, насколько полученный перевод человечен.

Например, языковая модель выбирает из двух услышанных словосочетаний "Let it be" & "Let eat bee" наиболее вероятное.

Важно при решении этой задачи научиться учиться без учителя. Активное направление -- перенос знаний, навыков (transfer learning). Основной смысл -- делаем общую модель, обучаем без учителя и затем подгоняем под конкретную задачу (fine-tuning).

Нам на вход даются тексты и мы их считаем реализациями случайной величины некоторого многомерного распределения. Итак, есть тексты `[t_1,t_2,...,t_k] ~ P(t_1,t_2,...,t_k)`. Применяем правило цепочки и разворачиваем вероятность: P(a,b,c)=P(a|b,c)P(b|c)P(c).

* Авторегрессионные модели -- один из подходов. Предсказываем вероятность следующего слова при наблюдении некоторого количества предыдущих слов. (autoregressive LM, ELMo, OpenAI GPT, XLNet).
* Предсказание слова в середине по остальным словам в тексте вокруг него. (Word2Vec CBOW, BERT, XLNet).

Рассмотрим простенькую авторегрессионную модель. Пусть `P(t_1,t_2,...,t_k)=П(P(t_i|t_{1:i-1}))`. При этом ограничим длину истории некоторым числом n сверху. А именно, скажем, что слова, которые встречались более n слов назад уже не влияют на вероятность слова на текущей позиции. Такие подходы и свойства называются Марковскими. Обучение заключается в подсчёте условных вероятностей.

Плюсы: простая реализация, быстрота.
Проблемы: 
* при увеличении n число N-грам растёт очень быстро, сложно обобщается и сжимается
* не можем работать с неизвестными словами (но если перейти к работе посимвольно, то отчасти ок, но вычислительная сложность растёт)
* не может учитывать сложного контекста

Прорывы:
* Skip-Gram -- предсказываем контекст по центральному слову (слева, справа)
* CBOW (continious bag of words) -- предсказываем центральное слово по контексту

Для каждого слова два вектора -- центральный и контекстный. Считаем по софтмаксу, применяем negative sampling, чтобы убыстрить. Функцией потерь выступает кросс-энтропия, число меток равно размеру словаря.

На обученной модели можно предсказывать правдоподобие текста по Skip-Gram.

Преимущества: 
* Простота
* Сжатое представление, можно использовать как признаки
* Возможность работы с большими словарями

Недостатки: 
* Out of vocabulary problem
* Один вектор для слова -- плохо, представление слов статично и не зависит от контекста

На помощь приходят авторегрессионные модели. RNNs, CNNs, LSTMs, Self-attention blocks.

Здесь обычно используем честный софтмакс, без негативного сэмплирования. Проблема OOV в этом случае решается взятием длины токена равного единице (то есть работаем с текстами на уровне одиночных символов). Но на практике такие модели (символьные) работают хуже, чем с токенами больших длин.

Один из подходов -- работать с представлениями на уровне целых слов, а вектора для неизвестных слов набирать из представлений N-грам. (FastText-embeddings).

Ещё применяют BPE (byte-pair encoding) -- типа сжатия без потерь для текстов, заменяя наиболее частые длинные участки новым символом. `aabc aabaa -> Xbc XbX -> Yc YX. Сжали без потерь в более чем два раза.

Нейросетевые модели позволяют получать сжатые представления слов с учётом контекста. Оказывается, первые слои выделяют морфологию, следующие -- поверхностные, а последние слои -- семантические. Абстракция и паттернинг. Учитывают сложные контекст и легко работают с OOV-words.

В качестве минусов: дорого и сложно обучать, сложно сходятся (RNNs), длина учитываемого контекста слева ограничена.

Чтобы учитывать контекст слева и справа от слова, применяют двунаправленные модели типа ELMa. Можно пойти дальше и юзать BERT -- тогда учитываться будет весь текст, выбираться некоторая маска входных слов, они будут заменены на фиктивные токены, предполагается их независимость (что не вполне корректно) и делается предсказание. В целом работает ОК.

Лучше, чем BERT работает XL Net. Итак:

* ELMo, OpenAI GPT раскручивают цепочку только слева-направо.
* раскручивает в обоих направлениях, но плохо, что есть предположение условной независимости этих выдернутых слов (то есть сами по себе ОНИ появляются независимо друг от друга, что, конечно, не так).
* XLNet - двунаправленный порядок, но порядок факторизации меняется.