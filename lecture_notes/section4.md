# 4. Языковые модели и генерация текста
## 4.1. RNNs (Recurrent Neural Networks), рекуррентные нейросети

Рабочая лошадка про обработке последовательностей данных — рекуррентная нейросеть.

Основная идея — есть некоторое внутреннее состояние, которое изменяется с каждым приходом порции входных данных.

* Считываем последовательность символ за символом или слово за словом и применяем к элементу, например, линейное преобразование.
* Вычисляем новое состояние ячейки памяти исходя из предыдущего состояния (того, что раньше было в памяти) и нового входного элемента.
* Вычисляем предсказание (выход) в текущем состоянии.

Потенциально мощнее Свёрточных нейросетей, однако завязаны на последовательном выполнении итераций (чтобы узнать текущее состояние, надо знать все предыдущие). Поэтому возможности для распаралеливания, а значит и скорость обработки на графических ускорителях куда меньше.

Не так хорошо обучаются, наблюдаются проблемы затухания и взрыва градиента (vanishing & exploding gradient).

Рассмотрим простой пример одномерной нейросети, состоящей из небольшого числа слоёв и будем решать задачу классификации для текстов.

![1D-RNN](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/simple_rnn.PNG "Example RNN architecture")