# 4. Языковые модели и генерация текста
## 4.1. RNNs (Recurrent Neural Networks), рекуррентные нейросети

Рабочая лошадка про обработке последовательностей данных — рекуррентная нейросеть.

Основная идея — есть некоторое внутреннее состояние, которое изменяется с каждым приходом порции входных данных.

* Считываем последовательность символ за символом или слово за словом и применяем к элементу, например, линейное преобразование.
* Вычисляем новое состояние ячейки памяти исходя из предыдущего состояния (того, что раньше было в памяти) и нового входного элемента.
* Вычисляем предсказание (выход) в текущем состоянии.

Потенциально мощнее Свёрточных нейросетей, однако завязаны на последовательном выполнении итераций (чтобы узнать текущее состояние, надо знать все предыдущие). Поэтому возможности для распаралеливания, а значит и скорость обработки на графических ускорителях куда меньше.

Не так хорошо обучаются, наблюдаются проблемы затухания и взрыва градиента (vanishing & exploding gradient).

Рассмотрим простой пример одномерной нейросети, состоящей из небольшого числа слоёв и будем решать задачу классификации для текстов.

![1D-RNN](https://raw.githubusercontent.com/xoxai/stepik-samsung-nlp-2019/master/lecture_notes/images/simple_rnn.PNG "Example RNN architecture")

Итак, на первом шаге `h0` мы имеем некоторый скаляр (рассматриваем одномерную сеть), это просто вход нейросети. На втором шаге имеем уже функцию от выхода на предыдущем шаге с некоторым весом `w` и входа на текущем шаге. `h_1=f(wh_0+z_1)`. На втором шаге уже имеем функцию от выхода предыдущего шага и текущего входа, то есть `h_2=f(wh_1+z_2)`. Это выражение можно раскрыть используя явное полное представление для состояния на шаге `h_1`. И так далее. В итоге мы получим довольно громоздкую композицию функций `h_i`.

Если теперь мы попытаемся пойти в обратную сторону (backprop), то по пути нам придётся взять производную от всех функций скрытых состояний `h_i` и веса будут возводиться в высокие степени. Таким образом приходим к двум проблемам -- взрывающийся и исчезающий (затухающий) градиент (exploding and vanishing gradient), поскольку производная от гиперболического тангенса, например, в промежутке от 0 до 1. Если перемножить между собой много таких штук, то получится очень маленькое число и значит до нашего некоторого слоя просто не дойдёт информация о ближних слоях. Градиент затухнет.

Проблема взрывающегося градиента проявляется, когда веса велики и набегают огромные значения, потому что вес возводится в высокую степень. Решается методом отсечения по пороговому значению и просто заменой на порог с соответствующим знаком. Проблема исчезающего градиента решается при помощи применения особых архитектур нейросетей -- блоков с т.н. long short-term memory (LSTM).

Основные идеи:

* сохранение постоянного объёма потока ошибки (сечения трубы)
* гейтинг (входной гейт, выходной гейт, направление изменения состояния, гейт памяти, гейт забывания, чувствительность).

Число параметров велико `8d^2+4d`, где d -- размерность скрытого состояния `h`. 

Упрощённая реализация идеи LSTM -- GRU (gated recurrent units). Число параметров `6d^2+6d`.

Ещё более простая модель -- Simple Recurrent Unit (SRU). Теперь каждая компонента вектора состояния на текущем шаге зависит только от одноименной с ней компонентой вектора предыдущего состояния (и не зависит от других). Примерно в 5-9 раз быстрее LSTM-ов при схожем качестве. Число параметров `3d^2+4d`.

## 4.2. Моделирование языка.