# 3. Базовые нейросетевые методы работы с текстами
## 3.5. Основные архитектуры нейросетей для задач NLP

Нейросети применяются для анализа текстов, поскольку особенность их обучения и построения позволяет добиться выучивания локальных особенностей текста и формулировок (паттернов), а также локальный контекст.

Также нейросети могут вычленять семантические отношения между словами, анализируя контекст. Математические методы, лежащие в их основе — матрицы и векторы (вообще говоря, чуть более общо — тензоры).

### Свёрточные блоки (Convolutional Neural Networks, CNNs)
* 1D свёртки
* инвариантны к переносу
* Эффективно вычисляются на гпу, хорошо распаралеливаются
* !НО недостаточно гибки для нахождения широких паттернов

### Рекуррентные нейросети
* Читаем символ за символом и предсказываем следующий 
* Помним состояния, поэтому это более естественно для анализа последовательностей
* позволяет выудить широкие паттерны
* !НО плохо учатся

Часто применяются агрегации или pooling'и:

* При локальной пулинге детали укрупняются, длина (размерность) уменьшается.
* При глобальном пулинге — получить вектор, не зависящий от длины текста

Прорывом в области нейросетей стало применение **механизма внимания** (attention mechanism, self-attemntion) — позволяет выделить наиболее значимые элементы последовательности. Можно рассматривать этот механизм как умную (адаптивную) агрегацию.

Архитектуры, которые не будут подробно рассматриваться в курсе, но о них полезно знать:
* **архитектуры с памятью** (memory Neural network, Neural Turing machine) не один вектор памяти, в много с возможностью выбора, откуда читать и куда записывать. 
    * позволяют учитывать сложный контекст
    * Потенциально (теоретически) могут быть применены для решения совершенно любых задач
    * !однако не очень хорошо и быстро сходятся на обучении
    * !ёмкость памяти невелика

* **рекурсивные нейросети** (tree-recursice NN). Используются для обхода многосвязных структур. Полезны при анализе синтаксических деревьев и агрегации конкретных узлов дерева. Можно применять для анализа языков со свободным порядком слов, но в настоящее время применяются нечасто.

* Графовые Свёрточные нейросети (graph Convolutional NN gcnns). Обобщение Свёрточных и рекуррентных сетей на произвольную структуру графа. Перспективное направление, активно исследуется. Но требует много вычислительных мощностей.

## 3.6. Свёрточные нейросети для обработки текстов.

**Свёртка** — математическая операция, принимающая на вход две непреравных или дискретных функции (сигнал и ядро) и на выходе отдающая третий, некий промежуточный сигнал. Характеризует степень похожести сигнала на ядро. Позволяет вычленять из последовательности паттерны.

Примерный алгоритм применения CNNs к задачам обработки текста:
* строим матрицу эмбедингов слов или символов (число столбцов — размерность эмбединга, а число строк — длина текста)
* по-очереди перебираем смещения и для каждого значения смещения свёртки выбираем k целых строк из входной матрицы, где k — размер ядра
* вытягиваем выбранные строки в один вектор из k*s компонент и находим скалярное произведение с ядром. Полученный результат заносим в ячейку новой матрицы.
* повторяем с новым значением смещения ядра и т.д.

Обычно блоки свёртки применяются вперемешку с блоками активации и агрегации. Делают это для того, чтобы расширить пятно восприятия, то есть получить возможность обрабатывать более длинные паттерны. Характеристика — ширина рецептивного поля.

`(k-1)d+1=n` -- формула для вычисления ширины перцептивного поля для свёрточной нейросети глубины `d` с ядром ширины `k`.

Как правило, свёрточные нейросети хорошо сходятся. Свёртки можно прореживать (dilated convolutions), это позволяет расширить пятно восприятия (рецептивное поле) при том, что число параметров останется тем же (или увеличится не так сильно, как при использовании полных свёрток). Если применять прореживание в первом слое НН, то это будет значить, что мы фактически выпускаем из виду (не применяем свёртку) к каждому второму слову. Это не то, что мы хотим, поэтому на первых слоях dilated convolutions не применяются, а на более глубоких -- да.

Авторегрессионные модели -- нечто среднее между свёрточными и рекуррентными нейросетями.